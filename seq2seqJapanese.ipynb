{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seqJapanese.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EbppAGvCCdWZ"
      ],
      "toc_visible": true,
      "mount_file_id": "1iCeN6jY743G9Ihl5qc4IRQWNa7Gr_fF1",
      "authorship_tag": "ABX9TyPQvokctaK828seTwcYaH+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NanaseSAITOH/DialogueBot/blob/master/seq2seqJapanese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQumV7IQ-nKo",
        "colab_type": "text"
      },
      "source": [
        "## データ準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV4J28m6_CzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pickle\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnsWSNOvGJcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data() :\n",
        "    %cd /content/drive/My Drive/Colab Notebooks/002_seq2seq_single_layer\n",
        "    #単語ファイルロード\n",
        "    with open('words.pickle', 'rb') as ff :\n",
        "        words=pickle.load(ff)         \n",
        "\n",
        "    #Encoder Inputデータをロード\n",
        "    with open('train_encoder.pickle', 'rb') as f :\n",
        "        encoder = pickle.load(f)\n",
        "\n",
        "    #Decoder Inputデータをロード\n",
        "    with open('train_decoder.pickle', 'rb') as g :\n",
        "        decoder = pickle.load(g)\n",
        "\n",
        "    #ラベルデータをロード\n",
        "    with open('label.pickle', 'rb') as h :\n",
        "        label = pickle.load(h)\n",
        "\n",
        "    #maxlenロード\n",
        "    with open('maxlen.pickle', 'rb') as maxlen :\n",
        "        [maxlen_e, maxlen_d] = pickle.load(maxlen)\n",
        "\n",
        "    #id2word\n",
        "    with open('indices2word.pickle', 'rb') as i2w :\n",
        "        indices2word = pickle.load(i2w)\n",
        "\n",
        "    #word2id\n",
        "    with open('word2indices.pickle', 'rb') as w2i :\n",
        "        word2indices = pickle.load(w2i)\n",
        "\n",
        "    print(word2indices[\"　\"])\n",
        "    row = encoder.shape[0]\n",
        "\n",
        "    encoder = encoder.reshape((row, maxlen_e))\n",
        "    decoder = decoder.reshape((row, maxlen_d))\n",
        "    label = label.reshape((row, maxlen_d))\n",
        "\n",
        "    data = {\n",
        "            'encoder'           :encoder,\n",
        "            'decoder'           :decoder,\n",
        "            'label'           :label,\n",
        "            'maxlen_e'    :maxlen_e,\n",
        "            'maxlen_d'    :maxlen_d,\n",
        "            'indices2word' : indices2word,\n",
        "            'word2indices' : word2indices,\n",
        "            'input_dim'   : len(words),\n",
        "            'output_dim'  : len(words)\n",
        "            }\n",
        "    return data"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDKemNkqFuud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "787e30b8-1ea3-418d-ce9b-3a3e8c92258f"
      },
      "source": [
        "dataset = load_data()\n",
        "maxlen_e     = dataset['maxlen_e']\n",
        "maxlen_d     = dataset['maxlen_d']\n",
        "encoder            = dataset['encoder']\n",
        "decoder            = dataset['decoder']\n",
        "label            = dataset['label'] \n",
        "indices2word = dataset['indices2word'] \n",
        "word2indices = dataset['word2indices'] \n",
        "data_row     = encoder.shape[0]                  # 訓練データの行数\n",
        "n_split      = int(data_row*0.9)           # データの分割比率\n",
        "# データを訓練用とテスト用に分割\n",
        "encoder_train, encoder_test = np.vsplit(encoder,[n_split])   #エンコーダインプット分割\n",
        "decoder_train, decoder_test = np.vsplit(decoder,[n_split])   #デコーダインプット分割\n",
        "label_train, label_test = np.vsplit(label,[n_split])   #ラベルデータ分割\n",
        "print(len(label_train))\n",
        "#train_dataset"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/002_seq2seq_single_layer\n",
            "7\n",
            "53311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJwEKTjY68lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# データをバッチ化するための関数\n",
        "def train2batch(input_data, output_data, batch_size):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    for i in range(0, len(input_data), batch_size):\n",
        "      input_batch.append(input_data[i:i+batch_size])\n",
        "      output_batch.append(output_data[i:i+batch_size])\n",
        "    return input_batch, output_batch"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2dQBYZKBrN4",
        "colab_type": "text"
      },
      "source": [
        "## NN定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSI5B8U77HlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "embedding_dim = 300 # 文字の埋め込み次元数\n",
        "hidden_dim = 256 # LSTMの隠れ層のサイズ\n",
        "vocab_size = 10169 # 扱う文字の数。今回は１３文字\n",
        "\n",
        "# GPU使う用\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Encoderクラス\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        embedding = self.word_embeddings(sequence)\n",
        "        # Many to Oneなので、第２戻り値を使う\n",
        "        _, state = self.lstm(embedding)\n",
        "        # state = (h, c)\n",
        "        return state"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzrdFfu07OOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoderクラス\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        # LSTMの128次元の隠れ層を13次元に変換する全結合層\n",
        "        self.hidden2linear = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, sequence, encoder_state):\n",
        "        embedding = self.word_embeddings(sequence)\n",
        "        # Many to Manyなので、第１戻り値を使う。\n",
        "        # 第２戻り値は推論時に次の文字を生成するときに使います。\n",
        "        output, state = self.lstm(embedding, encoder_state)\n",
        "        output = self.hidden2linear(output)\n",
        "        return output, state"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-Kn0-yS7RCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU使えるように。\n",
        "encoder = Encoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "decoder = Decoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "# 損失関数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 最適化\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFn9oJ7iBx0m",
        "colab_type": "text"
      },
      "source": [
        "## 学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84Eyl8R-7TpK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "fd80e8eb-11ae-4114-c073-040c1af7f529"
      },
      "source": [
        "BATCH_NUM = 32\n",
        "EPOCH_NUM = 100\n",
        "\n",
        "all_losses = []\n",
        "print(\"training ...\")\n",
        "for epoch in range(1, EPOCH_NUM+1):\n",
        "    epoch_loss = 0 # epoch毎のloss\n",
        "\n",
        "    # データをミニバッチに分ける\n",
        "    input_batch, output_batch = train2batch(encoder_train, decoder_train, batch_size=BATCH_NUM)\n",
        "    print(len(input_batch))\n",
        "    for i in range(len(input_batch)):\n",
        "\n",
        "        # 勾配の初期化\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        \n",
        "        # データをテンソルに変換\n",
        "        input_tensor = torch.tensor(input_batch[i], device=device, dtype=torch.long)\n",
        "        output_tensor = torch.tensor(output_batch[i], device=device, dtype=torch.long)\n",
        "\n",
        "        # Encoderの順伝搬\n",
        "        encoder_state = encoder(input_tensor)\n",
        "\n",
        "        # Decoderで使うデータはoutput_tensorを１つずらしたものを使う\n",
        "        # Decoderのインプットとするデータ\n",
        "        source = output_tensor[:, :-1]\n",
        "\n",
        "        # Decoderの教師データ\n",
        "        # 生成開始を表す\"_\"を削っている\n",
        "        target = output_tensor[:, 1:]\n",
        "\n",
        "        loss = 0\n",
        "        # 学習時はDecoderはこのように１回呼び出すだけでグルっと系列をループしているからこれでOK\n",
        "        # sourceが４文字なので、以下でLSTMが4回再帰的な処理してる\n",
        "        decoder_output, _ = decoder(source, encoder_state)\n",
        "        # decoder_output.size() = (100,4,13)\n",
        "        # 「13」は生成すべき対象の文字が13文字あるから。decoder_outputの3要素目は\n",
        "        # [-14.6240,  -3.7612, -11.0775,  ...,  -5.7391, -15.2419,  -8.6547]\n",
        "        # こんな感じの値が入っており、これの最大値に対応するインデックスを予測文字とみなす\n",
        "\n",
        "        for j in range(decoder_output.size()[1]):\n",
        "            # バッチ毎にまとめてloss計算\n",
        "            # 生成する文字は4文字なので、4回ループ\n",
        "            loss += criterion(decoder_output[:, j, :], target[:, j])\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        # 誤差逆伝播\n",
        "        loss.backward()\n",
        "        # パラメータ更新\n",
        "        # Encoder、Decoder両方学習\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "    # 損失を表示\n",
        "    print(\"Epoch %d: %.2f\" % (epoch, epoch_loss))\n",
        "    all_losses.append(epoch_loss)\n",
        "    if epoch_loss < 1: break\n",
        "print(\"Done\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training ...\n",
            "1666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-1258fa0ed6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# 誤差逆伝播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m# パラメータ更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Encoder、Decoder両方学習\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DXke5sM7XPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0QL2ukAHErL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(encoder.state_dict(), 'encoder.ckpt')\n",
        "torch.save(decoder.state_dict(), 'decoder.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbppAGvCCdWZ",
        "colab_type": "text"
      },
      "source": [
        "## テスト評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxN6R1cUImkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_batch, output_batch = train2batch(encoder_test, decoder_test, batch_size=BATCH_NUM)\n",
        "print(len(input_batch))\n",
        "with torch.no_grad(): # 勾配計算させない\n",
        "    input_tensor = torch.tensor(input_batch[0], device=device, dtype=torch.long)\n",
        "    encoder_state = encoder(input_tensor)\n",
        "\n",
        "    # Decoderにはまず文字列生成開始を表す\"_\"をインプットにするので、\"_\"のtensorをバッチサイズ分作成\n",
        "    start_char_batch = [[word2indices[\"SSSS\"]] for _ in range(BATCH_NUM)]\n",
        "    decoder_input = torch.tensor(start_char_batch, device=device, dtype=torch.long)\n",
        "\n",
        "    # 変数名変換\n",
        "    decoder_hidden = encoder_state\n",
        "\n",
        "    # バッチ毎の結果を結合するための入れ物を定義\n",
        "    batch_tmp = torch.zeros(100,1, dtype=torch.long, device=device)\n",
        "    # (100,1)\n",
        "    for i in range(50):\n",
        "        print(indices2word[input_tensor[7][i].item()])\n",
        "\n",
        "    for _ in range(5):\n",
        "      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "      # 予測文字を取得しつつ、そのまま次のdecoderのインプットとなる\n",
        "      decoder_input = get_max_index(decoder_output.squeeze())\n",
        "      print(indices2word[decoder_input[7][0].item()])\n",
        "      # バッチ毎の結果を予測順に結合\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}